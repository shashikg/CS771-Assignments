\documentclass[a4paper,11pt]{article}

\usepackage{mlsubmit}
\usepackage{listings}

\begin{document}

\initmlsubmision{3} % assignment number
{Shahsi Kant Gupta}   % your name
{160645}	% your roll number

\begin{mlsolution}
\noindent
Expanding the objective function we get:
\begin{equation}
  f(\alpha + \delta e_{n}) = (\alpha + \delta e_{n})^{T}1 - \frac{1}{2}(\alpha + \delta e_{n})^{T}G(\alpha + \delta e_{n})
\end{equation}
\begin{equation}
  f(\alpha + \delta e_{n}) = \alpha^{T}1 + \delta e_{n}^{T}1 - \frac{1}{2}(\alpha^{T}G\alpha + 2\delta e_{n}^{T}G\alpha + \delta^{2} e_{n}^{T}e_{n})
\end{equation}
\begin{equation}
  \frac{\partial{f(\alpha + \delta e_{n})}}{\partial{\delta}} = e_{n}^{T}1 - \frac{1}{2}(2e_{n}^{T}G\alpha + 2\delta e_{n}^{T}e_{n})
\end{equation} \\
Above equation should be equal to zero to maximize $f$ w.r.t $\delta$
\begin{equation}
  \Rightarrow e_{n}^{T}1 - e_{n}^{T}G\alpha - \delta e_{n}^{T}e_{n}) = 0
\end{equation}
\begin{equation}
  \Rightarrow 1 - \sum{\alpha_{i}G_{in}} - \delta = 0
\end{equation}
\begin{equation}
  \Rightarrow \delta_{*} = 1 - \sum{\alpha_{i}G_{in}}
\end{equation} \\
Now, this will not be the final answer as we need to project the $\delta_{*}$ as such that $\alpha$ should be in the range of $0 \leq \alpha_{n} \leq C$. Applying the conditions we get a project $\delta_{*}$ to be:
\begin{equation}
  \delta_{*} = min(C - \alpha_{n}, max(-\alpha_{n}, 1 - \sum{\alpha_{i}G_{in}}))
\end{equation} \\
\textbf{Pseudo Code/ Algorithm:}
\begin{lstlisting}[python]
initialise alpha
while converges:
  for i in range(N):
    delta = max(-alpha[n], 1 - dot(G, alpha))
    delta = min(C - alpha[n], delta)
    alpha[n] = alpha[n] + delta

\end{lstlisting}
\end{mlsolution}

\begin{mlsolution}
\noindent
Clearly, we can write:
\begin{equation}
  \sum_{n, m}||x_{n} - x_{m}||^{2} = \sum_{n, m}\{f_{n} = f_{m}\}||x_{n} - x_{m}||^{2}
                                   + \sum_{n, m}\{f_{n} \neq f_{m}\}||x_{n} - x_{m}||^{2}
\end{equation}
And since the data is fixed the LHS of the above equation will be constant. Therefore minimising the first term in RHS will maximise the second term of RHS as their sum need to be constant. The second term is nothing but the sum of squared distances between pairs of points in different clusters. Hence proved!


\end{mlsolution}

\begin{mlsolution}
\noindent
Part (1) \\
Using $ m = missed $ and $o = observed$.
\begin{equation}
  p(x_{n}^{m}|x_{n}^{o}, \mu, \Sigma) = p(x_{n}^{m}| \mu_{n}^{m|o}, \Sigma_{n}^{m|o} )
\end{equation}
Using the results from the book:
\begin{equation}
  \mu_{n}^{m|o} = \mu_{n}^{m} + \Sigma_{n}^{mo}(\Sigma_{n}^{oo})^{-1}(x_{n}^{o} - \mu_{n}^{o})
\end{equation}
\begin{equation}
  \Sigma_{n}^{m|o} = \Sigma_{n}^{mm} - \Sigma_{n}^{mo}(\Sigma_{n}^{oo})^{-1}\Sigma_{n}^{om}
\end{equation}

\noindent \\Part (2) \\
Taking $x_{n} = (x_{n}^{o}; x_{n}^{m})\implies E[x_{n}] = (x_{n}^{o}; E[x_{n}^{m}])$. Therefore, our expected CLL can be easily written as:
\begin{equation}
  Q(\theta, \theta^{t-1}) = \sum_{n=1}^{N} log(N(E[x_{n}]|\mu, \Sigma))
\end{equation}
\\Part (3) \\
Solving the above MLE will yield:
\begin{equation}
  \mu = \frac{1}{N} \sum_{n=1}^{N}E[x_{n}]
\end{equation}
\begin{equation}
  \Sigma = \frac{1}{N} \sum_{n=1}^{N}E[x_{n}x_{n}^{T}] - \mu\mu^{T}
\end{equation}
\\
\textbf{EM Algorithm:}
\\ Initialise u, sigma\\
E Step: Compute $\mu_{n}^{m|o}$ and $\Sigma_{n}^{m|o}$ for each n.\\
\begin{equation}
  \mu_{n}^{m|o} = \mu_{n}^{m} + \Sigma_{n}^{mo}(\Sigma_{n}^{oo})^{-1}(x_{n}^{o} - \mu_{n}^{o})
\end{equation}
\begin{equation}
  \Sigma_{n}^{m|o} = \Sigma_{n}^{mm} - \Sigma_{n}^{mo}(\Sigma_{n}^{oo})^{-1}\Sigma_{n}^{om}
\end{equation}
\\
M Step: re-estimate u, sigma via MLE
\begin{equation}
  \mu = \frac{1}{N} \sum_{n=1}^{N}E[x_{n}]
\end{equation}
\begin{equation}
  \Sigma = \frac{1}{N} \sum_{n=1}^{N}E[x_{n}x_{n}^{T}] - \mu\mu^{T}
\end{equation}
If doesnt converge go back to E Step!

\end{mlsolution}

\begin{mlsolution}
\noindent
This problem can be simply solved by taking it to be similar to clustering problem, where we have added advantage of estimating the parameters by using actuall datas during the M step of EM Algorithm. \\ \\
Taking $y_{n+m}^{'}$ to be the new class data, we can write $y_{n+m}^{'} = (y_{n}; z_{m}) \implies y_{(n+m)k}^{'} = (y_{nk}; z_{mk})$. Let the $n+m = i; 0 \leq i \leq N+M$. We can write $E[y_{ik}^{'}] = (y_{nk}; E[z_{mk}])$. Therefore, our new CLL can be written as:
\begin{equation}
  Q(\theta, \theta^{old}) = \sum_{i=1}^{N+M}\sum_{k=1}^{K} E[y_{ik}^{'}][log(\pi_{k}) + log(N(x_{n}|\mu_{k}, \Sigma_{k}))
\end{equation}
\textbf{EM Algorithm:} \\
(1) Initialise: $\{\pi_{k}, \mu_{k}, \Sigma_{k}\}_{k=1}^{K}$\\
(2) E Step: Compute expectation of $z_{m}$ for all $m, k$
\begin{equation}
  E[z_{mk}] = \frac{\pi_{k}N(x_{(N+m)}|\mu_{k},\Sigma_{k})}{\sum_{l=1}^{K}\pi_{l}N(x_{(N+m)}|\mu_{l},\Sigma_{l})}
\end{equation}
(3) M Step: re-estimate parameters via MLE
\begin{equation}
  N_{k} = \sum_{i=1}^{N+M}E[y_{ik}^{'}]
\end{equation}
\begin{equation}
  \mu_{k} = \frac{1}{N_{k}}\sum_{i=1}^{N+M}E[y_{ik}^{'}]x_{i}
\end{equation}
\begin{equation}
  \Sigma_{k} = \frac{1}{N_{k}}\sum_{i=1}^{N+M}E[y_{ik}^{'}](x_{i} - \mu_{k})(x_{i} - \mu_{k})^{T}
\end{equation}
\begin{equation}
  \pi_{k} = \frac{N_{k}}{N+M}
\end{equation}
(4) Go to step 2 if not yet converged!

\end{mlsolution}

\begin{mlsolution}
\noindent
\textbf{Part (1)}\\
A standard linear model will only work for those solutions where we have to regress a linear curve whereas this model can be a combination of K different linear curves basically what the model is doing is that at first it is clustering the data on k different linear curves and then the prediction are made for y. This will also help in the reduction of outliers in a linear curve as the outliers may get separate out due to clustering. \\ \\
\textbf{Part (2 and 3)}\\
Here, our latent variable model becomes:
\begin{equation}
  p(z_{n}=k|y_{n},\theta) = \frac{p(z_{n}=k)p(y_{n}|z_{n}=k, \theta)}{\sum_{l=1}^{K}p(z_{n}=l)p(y_{n}|z_{n}=l, \theta)}
\end{equation}
\begin{equation}
  p(y_{n}, z_{n}|\theta) = p(y_{n}|z_{n},\theta)p(z_{n}|\theta)
\end{equation}
where:
\begin{equation}
  p(z_{n}=k) = \pi_{k}
\end{equation}
\begin{equation}
  p(y_{n}|z_{n},\theta) = N(w_{z_{n}}^{T}x_{n}, \beta^{-1})
\end{equation} \\
\textbf{ALT-OPT Algorithm}\\
Step 1 find the best $z_{n}$: \\
\begin{equation}
  z_{n} = argmax_{z_{n}}\frac{\pi_{k}N(w_{z_{n}}^{T}x_{n}, \beta^{-1})}{\sum_{l=1}^{K}\pi_{l}N(w_{l}^{T}x_{n}, \beta^{-1})}
\end{equation}
\begin{equation}
  \implies z_{n} = argmax_{z_{n}}\frac{\pi_{k}exp(\frac{-\beta}{2}(y_{n}-w_{z_{n}}^{T}x_{n})^{2})}{\sum_{l=1}^{K}\pi_{l}exp(\frac{-\beta}{2}(y_{n}-w_{l}^{T}x_{n})^{2})}
\end{equation}
Step 2 re-estimate the parameters: \\
\begin{equation}
  N_{k} = \sum_{n=1}^{N}z_{nk}
\end{equation}
\begin{equation}
  w_{k} = (X_{k}^{T}X_{k})^{-1}X_{k}^{T}y_{k}
\end{equation}
\begin{equation}
  \pi_{k} = N_{k}/N
\end{equation}
Here $X_{k}$ are $N_{k}$x$D$ matrix containing training sets which is clustered in class k. And $y_{n}$ are $N_{k}$x$1$ vectors containing training sets labels which is clustered in class k. \\ \\
If $\pi_{k} = 1/K$ then:
\begin{equation}
 z_{n} = argmax_{z_{n}}\frac{exp(\frac{-\beta}{2}(y_{n}-w_{z_{n}}^{T}x_{n})^{2})}{\sum_{l=1}^{K}exp(\frac{-\beta}{2}(y_{n}-w_{l}^{T}x_{n})^{2})}
\end{equation}
This update is equivalent to multi output logistic regression. \\ \\
\noindent
\textbf{EM Algorithm}\\
E Step find $p(z_{k})$ for each k: \\
\begin{equation}
  p(z_{n}=k|y_{n},\theta) = \frac{\pi_{k}N(w_{k}^{T}x_{n}, \beta^{-1})}{\sum_{l=1}^{K}\pi_{l}N(w_{l}^{T}x_{n}, \beta^{-1})}
\end{equation}
\begin{equation}
  \implies p(z_{n}=k|y_{n},\theta) = \frac{\pi_{k}exp(\frac{-\beta}{2}(y_{n}-w_{k}^{T}x_{n})^{2})}{\sum_{l=1}^{K}\pi_{l}exp(\frac{-\beta}{2}(y_{n}-w_{l}^{T}x_{n})^{2})}
\end{equation}
M Step re-estimate the parameters: \\
\begin{equation}
  N_{k} = \sum_{n=1}^{N}E[z_{nk}]
\end{equation}
\begin{equation}
  w_{k} = (X_{k}^{T}X_{k})^{-1}X_{k}^{T}y_{k}
\end{equation}
\begin{equation}
  \pi_{k} = N_{k}/N
\end{equation}
Here $X_{k}$ are $N_{k}$x$D$ matrix containing training sets which is clustered in class k. And $y_{n}$ are $N_{k}$x$1$ vectors containing training sets labels which is clustered in class k. \\ \\
As $\beta \rightarrow \infty$:
\begin{equation}
  p(z_{n}=k|y_{n},\theta) \rightarrow \pi_{k}
\end{equation}
Therefore, $E[z_{nk}] = z_{nk}$ which means EM reduces to ALT-OPT


\end{mlsolution}

\begin{mlsolution}
\noindent
\textbf{Problem 1: \\Part (1)}\\
As we increase the regularisation hyperparameter error increases! The possible reason is that the train set and test sets both seems to be taken from a same sin curve without much outliers so less the regularisation better curve toward train data, indirectly better curve toward test data.\\ \\
\textbf{Part (2)}\\
Lesser the value of L more the error in prediction, the reason being that less number feature point taken. The value of L=50 is good enough as increasing L to 100 changes the rmse by just 0.003! \\ \\
\textbf{Problem 2: \\Part (1)}\\
After plotting the datas it can be easily seen that the clusters are radially distributed around origin. Therefore for the hand crafted part feature transformed was used to be the distance from origin.\\ \\
\textbf{Part (2)}\\
As the given data can be easily clustered if we use a feature transform which based on distance of the point from origin. One landmark sometime clusters the data well when the choosen landmark is closer to the origin whereas when it is farther to the origin it doesnt cluster well because the datas are distributed radially around the origin! \\ \\
\textbf{Note:} landmark points are shown in red colours with star mark as the blue color points where not visible whent the lanmark belongs to the blue clusters

\end{mlsolution}


\end{document}
